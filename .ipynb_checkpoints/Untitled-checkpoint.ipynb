{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omid Heravi  \n",
    "December 27th, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.1 Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inusrence companies can and will use Machine learning more and more frequently in the near future. As a data intensive industry, Insurence companies can all benefit from the use of Machine learning techniques in processing insurence claims. As per Allstate's challenge, we will use several machine learing techniques with the data provided to conduct data analysis and predict the reuqested attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope of this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will investigate using Machine Learning Tecinques to predict the requested loss column/attribute. Each row in the data corresponds to an insurence claim. Each column/attribute corresponds with a feature that will help us in data analysis process to better predict the 'loss' attribte. Despite the fact that all columns are ambigious, the 'loss' columns does not explictly tell us anything about what we're predicting, however those details are not necessary in the grand scheme of analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Insurence is an interesting domain of Machine learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There are many instances where we or our loved ones have been involved in an accident, tragedy or etc. and which is when we all rely on ur insuence provider for relief and help. The importance of this issue is very personal to me as my father who recently was involved in accident would have benefited from an speedy process. Machine learning can quickly improve the process, effiecenly and productivity of every person involved during an insurence claim process. \n",
    "\n",
    "2. Machine learning can help us uncover hidden connections and patterns we had never realized before. This can include better indicators for prevention of accidents among individuals, lowering the cost of insurence due to automation and efficiency, better policy desinging and so many more. \n",
    "\n",
    "3. As a data intensive industry, Machine learning is poised to take over the insurence claim analysis pipeline thus proving to be a formidable business venture for many entrepreneurs and problem solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aim of this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to get a feel for which features are better indicators are of an insurence claim severity, as mentioned in the challenge page. Predicting the severity of an insurence claim is very difficult yet with the data provided, we hope to as accurately predict which features and which model perform the best when it comes to predicting the severity of an insurence claim. More details in Problem Statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Used in this Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two primary Datasets for this project:\n",
    "\n",
    "1. Train.csv = train Data provided by Allstate Challenge. The original Shape of the data is in (188318, 131). This dataset also includes the 'loss' column by which we'll be completing the challenge.\n",
    "\n",
    "2. test.csv = test data provided by the challenge. The original shape of the data is in (125546, 130). This datset however does not include the 'loss' column. also train set does not include as munch insurance isntance claims as much as the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2 Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Model that can predict the severity of an insurence claim:\n",
    "\n",
    "<table>\n",
    "<th>Category</th><th>Details</th>\n",
    "<tr><td>Input</td><td> The original 129 attributes/features (exclusing the 'ID' and 'Loss' columns). once again, due to the ambiguity of the feature set, we cannot further intrepet what is our input, however we can assume that data such as date, location, estimated cost, type and etc.</td></tr>\n",
    "<tr><td>Output</td><td>Prdicted value/severity of the insurence claim. The output will be labeled 'loss'. Once again due to ambiguity of the overall dataset, we assume 'loss' is basically the integer value of a claim's seveirty.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting characteristics of this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are a few inetresting key points I would like to point out: *\n",
    " 1. The data is a mix of both categorical and continious data. \n",
    " 2. The 'loss' column is heavily skewed and will need to be transformed.\n",
    " 3. In fact all the data is already provided, so there is no need for data mining, cleaning and or etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This challenge requires heavy duty data exploartion just so you can wrap your head around the given dataset since most of the columns are ambigious.\n",
    "2. A simple solution can often be the best solution despite the fact that it's a simple solution even though one might be tempted to try an over-the-top solution/algo/model which is quite unnecesary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is a simple regression problem since we're asked to predict the 'loss'/severity attribute as a result of the other attributes. It is not initally obvious which kind of model will perform best.\n",
    "\n",
    "Charahcteristics of the problem:\n",
    "\n",
    "* continuous data\n",
    "* categorical data\n",
    "* Noisy data\n",
    "* ambigious features\n",
    "* Regression Problem\n",
    "* Predicting one single attribute "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Understanding the Dataset **\n",
    " * Data Shape \n",
    " * Skew \n",
    " * Mean, STD deviation, Max, etc. \n",
    " \n",
    "** Data Visualization **\n",
    " * Scatter Plots\n",
    " * Correlation\n",
    " * Density Plots\n",
    " \n",
    "** Data Processing ** \n",
    " * Transformation\n",
    " * One-Hot encoding\n",
    " * Train/Test Data preparation\n",
    " \n",
    "** Model Evalaution **\n",
    " * Algorithm implementations\n",
    " * Model Evaluations\n",
    " * Model Predicitons\n",
    " * Model Analysis\n",
    " \n",
    "** Appendix **\n",
    " * Conclusion\n",
    " * Personal Thoughts\n",
    " * Thanks and Appreciations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution will be in a .csv format where the predicted 'loss'/severty value will be stored. For each insurence claim 'ID', it will follow with a 'loss' value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.3 Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As requested by the kaggle challenge its self, we will be using **MEAN ABSOLUTE ERROR** or also known as **MAE**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./mae3.jpg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mean Absolute Error as a metric is ideal for most of simple regression problems, hence why it has been requested of us to use it. MAE can covey very usefu informaiton. The formula takes the avergae of the absolute difference between the predicted label value and the actual label value. This makes it much easier to interpret incomparison to other alternative regression error metrics such as MSE, RMSE, and etc. [JJ](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d) explains in detail the beautiful simplicity and ease of interpreation of MAE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.1 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Primary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided dataset is in two sets. Train and Test datasets. Both vary. The train set has a lot more insurence claim instances (rows) than the test set. It also includes the 'loss' attribute hence why we'll be predicting the 'loss' value. The datasets both contain categorical and continious data. With 129 actual attributes/features, and ~188000 rows, the training dataset is vast. \n",
    "\n",
    "Some key facts about the dataset:\n",
    "* Categirical data is in 116 columns\n",
    "* The categorical data is quite large, and will require one-hot encoding \n",
    "* Continious data set is about 15 columns\n",
    "* This is fairly easy to process and will need to further transformation \n",
    "* 'Loss' or the label to be predicted is heavily skewed and will require log-transformation\n",
    "* No data is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Descriptive satistics:\n",
    "The descriptive statistics does not prove to be very useful, however it is a great method to understand the variance in our data. \n",
    "\n",
    "**Skewness:** \n",
    "\n",
    "|Column   |Skewness|\n",
    "|---------|:------:|\n",
    "|cont1    |0.516424|\n",
    "|cont2    |-0.31094|\n",
    "|cont3    |-0.01000|\n",
    "|cont4    |0.416096|\n",
    "|cont5    |0.681622|\n",
    "|cont6    |0.461214|\n",
    "|cont7    |0.826053|\n",
    "|cont8    |0.676634|\n",
    "|cont9    |1.072429|\n",
    "|cont10   |0.355001|\n",
    "|cont11   |0.280821|\n",
    "|cont12   |0.291992|\n",
    "|cont13   |0.380742|\n",
    "|cont14   |0.248674|\n",
    "|loss     |3.794958|\n",
    "\n",
    "**Descriptive Stats:**\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-yw4l{vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-yw4l\"></th>\n",
    "    <th class=\"tg-yw4l\">cont1</th>\n",
    "    <th class=\"tg-yw4l\">cont2</th>\n",
    "    <th class=\"tg-yw4l\">cont3</th>\n",
    "    <th class=\"tg-yw4l\">cont4</th>\n",
    "    <th class=\"tg-yw4l\">cont5<br></th>\n",
    "    <th class=\"tg-yw4l\">cont6</th>\n",
    "    <th class=\"tg-yw4l\">cont7</th>\n",
    "    <th class=\"tg-yw4l\">cont8</th>\n",
    "    <th class=\"tg-yw4l\">cont9</th>\n",
    "    <th class=\"tg-yw4l\">cont10</th>\n",
    "    <th class=\"tg-yw4l\">cont11</th>\n",
    "    <th class=\"tg-yw4l\">cont12</th>\n",
    "    <th class=\"tg-yw4l\">cont13</th>\n",
    "    <th class=\"tg-yw4l\">cont14</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">count</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">mean</td>\n",
    "    <td class=\"tg-yw4l\">0.493861</td>\n",
    "    <td class=\"tg-yw4l\">0.507188</td>\n",
    "    <td class=\"tg-yw4l\">0.498918</td>\n",
    "    <td class=\"tg-yw4l\">0.491812</td>\n",
    "    <td class=\"tg-yw4l\">0.487428</td>\n",
    "    <td class=\"tg-yw4l\">0.207202</td>\n",
    "    <td class=\"tg-yw4l\">0.490945</td>\n",
    "    <td class=\"tg-yw4l\">0.484970</td>\n",
    "    <td class=\"tg-yw4l\">0.486437</td>\n",
    "    <td class=\"tg-yw4l\">0.498066</td>\n",
    "    <td class=\"tg-yw4l\">0.493511</td>\n",
    "    <td class=\"tg-yw4l\">0.49315</td>\n",
    "    <td class=\"tg-yw4l\">0.493138</td>\n",
    "    <td class=\"tg-yw4l\">0.495717</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">std</td>\n",
    "    <td class=\"tg-yw4l\">0.187640</td>\n",
    "    <td class=\"tg-yw4l\">0.207202</td>\n",
    "    <td class=\"tg-yw4l\">0.202105</td>\n",
    "    <td class=\"tg-yw4l\">0.211292</td>\n",
    "    <td class=\"tg-yw4l\">0.209027</td>\n",
    "    <td class=\"tg-yw4l\">0.205273</td>\n",
    "    <td class=\"tg-yw4l\">0.178450</td>\n",
    "    <td class=\"tg-yw4l\">0.199370</td>\n",
    "    <td class=\"tg-yw4l\">0.181660</td>\n",
    "    <td class=\"tg-yw4l\">0.185877</td>\n",
    "    <td class=\"tg-yw4l\">0.209737</td>\n",
    "    <td class=\"tg-yw4l\">0.209427</td>\n",
    "    <td class=\"tg-yw4l\">0.212777</td>\n",
    "    <td class=\"tg-yw4l\">0.222488</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">min</td>\n",
    "    <td class=\"tg-yw4l\">0.000016</td>\n",
    "    <td class=\"tg-yw4l\">0.001149</td>\n",
    "    <td class=\"tg-yw4l\">0.002634</td>\n",
    "    <td class=\"tg-yw4l\">0.176921</td>\n",
    "    <td class=\"tg-yw4l\">0.281143</td>\n",
    "    <td class=\"tg-yw4l\">0.012683</td>\n",
    "    <td class=\"tg-yw4l\">0.069503</td>\n",
    "    <td class=\"tg-yw4l\">0.236880</td>\n",
    "    <td class=\"tg-yw4l\">0.000080</td>\n",
    "    <td class=\"tg-yw4l\">0.000000</td>\n",
    "    <td class=\"tg-yw4l\">0.035321</td>\n",
    "    <td class=\"tg-yw4l\">0.036232</td>\n",
    "    <td class=\"tg-yw4l\">0.000228</td>\n",
    "    <td class=\"tg-yw4l\">0.179722</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">25%</td>\n",
    "    <td class=\"tg-yw4l\">0.346090</td>\n",
    "    <td class=\"tg-yw4l\">0.358319</td>\n",
    "    <td class=\"tg-yw4l\">0.336963</td>\n",
    "    <td class=\"tg-yw4l\">0.327354</td>\n",
    "    <td class=\"tg-yw4l\">0.281143</td>\n",
    "    <td class=\"tg-yw4l\">0.336105</td>\n",
    "    <td class=\"tg-yw4l\">0.350175</td>\n",
    "    <td class=\"tg-yw4l\">0.312800</td>\n",
    "    <td class=\"tg-yw4l\">0.358970</td>\n",
    "    <td class=\"tg-yw4l\">0.364580</td>\n",
    "    <td class=\"tg-yw4l\">0.310961</td>\n",
    "    <td class=\"tg-yw4l\">0.311661</td>\n",
    "    <td class=\"tg-yw4l\">0.315758</td>\n",
    "    <td class=\"tg-yw4l\">0.294610</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">50%</td>\n",
    "    <td class=\"tg-yw4l\">0.475784</td>\n",
    "    <td class=\"tg-yw4l\">0.555782</td>\n",
    "    <td class=\"tg-yw4l\">0.527991</td>\n",
    "    <td class=\"tg-yw4l\">0.452887</td>\n",
    "    <td class=\"tg-yw4l\">0.422268</td>\n",
    "    <td class=\"tg-yw4l\">0.440945</td>\n",
    "    <td class=\"tg-yw4l\">0.438285</td>\n",
    "    <td class=\"tg-yw4l\">0.441060</td>\n",
    "    <td class=\"tg-yw4l\">0.441450</td>\n",
    "    <td class=\"tg-yw4l\">0.461190</td>\n",
    "    <td class=\"tg-yw4l\">0.457203</td>\n",
    "    <td class=\"tg-yw4l\">0.462286</td>\n",
    "    <td class=\"tg-yw4l\">0.363547</td>\n",
    "    <td class=\"tg-yw4l\">0.407403</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">75%</td>\n",
    "    <td class=\"tg-yw4l\">0.623912</td>\n",
    "    <td class=\"tg-yw4l\">0.681761</td>\n",
    "    <td class=\"tg-yw4l\">0.634224</td>\n",
    "    <td class=\"tg-yw4l\">0.652072</td>\n",
    "    <td class=\"tg-yw4l\">0.643315</td>\n",
    "    <td class=\"tg-yw4l\">0.655021</td>\n",
    "    <td class=\"tg-yw4l\">0.591045</td>\n",
    "    <td class=\"tg-yw4l\">0.623580</td>\n",
    "    <td class=\"tg-yw4l\">0.566820</td>\n",
    "    <td class=\"tg-yw4l\">0.614590</td>\n",
    "    <td class=\"tg-yw4l\">0.678924</td>\n",
    "    <td class=\"tg-yw4l\">0.675759</td>\n",
    "    <td class=\"tg-yw4l\">0.689974</td>\n",
    "    <td class=\"tg-yw4l\">0.724623</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">max</td>\n",
    "    <td class=\"tg-yw4l\">0.984975</td>\n",
    "    <td class=\"tg-yw4l\">0.862654</td>\n",
    "    <td class=\"tg-yw4l\">0.944251</td>\n",
    "    <td class=\"tg-yw4l\">0.954297</td>\n",
    "    <td class=\"tg-yw4l\">0.983674</td>\n",
    "    <td class=\"tg-yw4l\">0.997162</td>\n",
    "    <td class=\"tg-yw4l\">1.000000</td>\n",
    "    <td class=\"tg-yw4l\">0.980200</td>\n",
    "    <td class=\"tg-yw4l\">0.995400</td>\n",
    "    <td class=\"tg-yw4l\">0.994980</td>\n",
    "    <td class=\"tg-yw4l\">0.998742</td>\n",
    "    <td class=\"tg-yw4l\">0.998484</td>\n",
    "    <td class=\"tg-yw4l\">0.988494</td>\n",
    "    <td class=\"tg-yw4l\">0.844848</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Descriptive satistics:\n",
    "The same analysis could be applied to Categorical dataset as well. Allbeit, since it is categorical data, Descrptive statistics wont be the same. Niether will there be any inherent 'skewness'. Thus the following data is appropriate:\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>cat1</th>\n",
    "      <th>cat2</th>\n",
    "      <th>cat3</th>\n",
    "      <th>cat4</th>\n",
    "      <th>cat5</th>\n",
    "      <th>cat6</th>\n",
    "      <th>cat7</th>\n",
    "      <th>cat8</th>\n",
    "      <th>cat9</th>\n",
    "      <th>cat10</th>\n",
    "      <th>cat11</th>\n",
    "      <th>cat12</th>\n",
    "      <th>cat13</th>\n",
    "      <th>cat14</th>\n",
    "      <th>cat15</th>\n",
    "      <th>cat16</th>\n",
    "      <th>cat17</th>\n",
    "      <th>cat18</th>\n",
    "      <th>cat19</th>\n",
    "      <th>cat20</th>\n",
    "      <th>cat21</th>\n",
    "      <th>cat22</th>\n",
    "      <th>cat23</th>\n",
    "      <th>cat24</th>\n",
    "      <th>cat25</th>\n",
    "      <th>cat26</th>\n",
    "      <th>cat27</th>\n",
    "      <th>cat28</th>\n",
    "      <th>cat29</th>\n",
    "      <th>cat30</th>\n",
    "      <th>cat31</th>\n",
    "      <th>cat32</th>\n",
    "      <th>cat33</th>\n",
    "      <th>cat34</th>\n",
    "      <th>cat35</th>\n",
    "      <th>cat36</th>\n",
    "      <th>cat37</th>\n",
    "      <th>cat38</th>\n",
    "      <th>cat39</th>\n",
    "      <th>cat40</th>\n",
    "      <th>cat41</th>\n",
    "      <th>cat42</th>\n",
    "      <th>cat43</th>\n",
    "      <th>cat44</th>\n",
    "      <th>cat45</th>\n",
    "      <th>cat46</th>\n",
    "      <th>cat47</th>\n",
    "      <th>cat48</th>\n",
    "      <th>cat49</th>\n",
    "      <th>cat50</th>\n",
    "      <th>cat51</th>\n",
    "      <th>cat52</th>\n",
    "      <th>cat53</th>\n",
    "      <th>cat54</th>\n",
    "      <th>cat55</th>\n",
    "      <th>cat56</th>\n",
    "      <th>cat57</th>\n",
    "      <th>cat58</th>\n",
    "      <th>cat59</th>\n",
    "      <th>cat60</th>\n",
    "      <th>cat61</th>\n",
    "      <th>cat62</th>\n",
    "      <th>cat63</th>\n",
    "      <th>cat64</th>\n",
    "      <th>cat65</th>\n",
    "      <th>cat66</th>\n",
    "      <th>cat67</th>\n",
    "      <th>cat68</th>\n",
    "      <th>cat69</th>\n",
    "      <th>cat70</th>\n",
    "      <th>cat71</th>\n",
    "      <th>cat72</th>\n",
    "      <th>cat73</th>\n",
    "      <th>cat74</th>\n",
    "      <th>cat75</th>\n",
    "      <th>cat76</th>\n",
    "      <th>cat77</th>\n",
    "      <th>cat78</th>\n",
    "      <th>cat79</th>\n",
    "      <th>cat80</th>\n",
    "      <th>cat81</th>\n",
    "      <th>cat82</th>\n",
    "      <th>cat83</th>\n",
    "      <th>cat84</th>\n",
    "      <th>cat85</th>\n",
    "      <th>cat86</th>\n",
    "      <th>cat87</th>\n",
    "      <th>cat88</th>\n",
    "      <th>cat89</th>\n",
    "      <th>cat90</th>\n",
    "      <th>cat91</th>\n",
    "      <th>cat92</th>\n",
    "      <th>cat93</th>\n",
    "      <th>cat94</th>\n",
    "      <th>cat95</th>\n",
    "      <th>cat96</th>\n",
    "      <th>cat97</th>\n",
    "      <th>cat98</th>\n",
    "      <th>cat99</th>\n",
    "      <th>cat100</th>\n",
    "      <th>cat101</th>\n",
    "      <th>cat102</th>\n",
    "      <th>cat103</th>\n",
    "      <th>cat104</th>\n",
    "      <th>cat105</th>\n",
    "      <th>cat106</th>\n",
    "      <th>cat107</th>\n",
    "      <th>cat108</th>\n",
    "      <th>cat109</th>\n",
    "      <th>cat110</th>\n",
    "      <th>cat111</th>\n",
    "      <th>cat112</th>\n",
    "      <th>cat113</th>\n",
    "      <th>cat114</th>\n",
    "      <th>cat115</th>\n",
    "      <th>cat116</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>count</th>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "      <td>188318</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>unique</th>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>3</td>\n",
    "      <td>3</td>\n",
    "      <td>3</td>\n",
    "      <td>3</td>\n",
    "      <td>4</td>\n",
    "      <td>4</td>\n",
    "      <td>4</td>\n",
    "      <td>4</td>\n",
    "      <td>4</td>\n",
    "      <td>4</td>\n",
    "      <td>4</td>\n",
    "      <td>4</td>\n",
    "      <td>4</td>\n",
    "      <td>4</td>\n",
    "      <td>4</td>\n",
    "      <td>4</td>\n",
    "      <td>8</td>\n",
    "      <td>7</td>\n",
    "      <td>8</td>\n",
    "      <td>7</td>\n",
    "      <td>5</td>\n",
    "      <td>7</td>\n",
    "      <td>5</td>\n",
    "      <td>8</td>\n",
    "      <td>7</td>\n",
    "      <td>5</td>\n",
    "      <td>16</td>\n",
    "      <td>15</td>\n",
    "      <td>19</td>\n",
    "      <td>9</td>\n",
    "      <td>13</td>\n",
    "      <td>17</td>\n",
    "      <td>20</td>\n",
    "      <td>17</td>\n",
    "      <td>20</td>\n",
    "      <td>11</td>\n",
    "      <td>84</td>\n",
    "      <td>131</td>\n",
    "      <td>16</td>\n",
    "      <td>51</td>\n",
    "      <td>61</td>\n",
    "      <td>19</td>\n",
    "      <td>23</td>\n",
    "      <td>326</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>top</th>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>D</td>\n",
    "      <td>B</td>\n",
    "      <td>B</td>\n",
    "      <td>D</td>\n",
    "      <td>D</td>\n",
    "      <td>B</td>\n",
    "      <td>B</td>\n",
    "      <td>C</td>\n",
    "      <td>B</td>\n",
    "      <td>B</td>\n",
    "      <td>B</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>D</td>\n",
    "      <td>D</td>\n",
    "      <td>C</td>\n",
    "      <td>E</td>\n",
    "      <td>C</td>\n",
    "      <td>A</td>\n",
    "      <td>P</td>\n",
    "      <td>F</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>A</td>\n",
    "      <td>E</td>\n",
    "      <td>E</td>\n",
    "      <td>G</td>\n",
    "      <td>F</td>\n",
    "      <td>B</td>\n",
    "      <td>BI</td>\n",
    "      <td>CL</td>\n",
    "      <td>A</td>\n",
    "      <td>E</td>\n",
    "      <td>BM</td>\n",
    "      <td>A</td>\n",
    "      <td>K</td>\n",
    "      <td>HK</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>freq</th>\n",
    "      <td>141550</td>\n",
    "      <td>106721</td>\n",
    "      <td>177993</td>\n",
    "      <td>128395</td>\n",
    "      <td>123737</td>\n",
    "      <td>131693</td>\n",
    "      <td>183744</td>\n",
    "      <td>177274</td>\n",
    "      <td>113122</td>\n",
    "      <td>160213</td>\n",
    "      <td>168186</td>\n",
    "      <td>159825</td>\n",
    "      <td>168851</td>\n",
    "      <td>186041</td>\n",
    "      <td>188284</td>\n",
    "      <td>181843</td>\n",
    "      <td>187009</td>\n",
    "      <td>187331</td>\n",
    "      <td>186510</td>\n",
    "      <td>188114</td>\n",
    "      <td>187905</td>\n",
    "      <td>188275</td>\n",
    "      <td>157445</td>\n",
    "      <td>181977</td>\n",
    "      <td>169969</td>\n",
    "      <td>177119</td>\n",
    "      <td>168250</td>\n",
    "      <td>180938</td>\n",
    "      <td>184593</td>\n",
    "      <td>184760</td>\n",
    "      <td>182980</td>\n",
    "      <td>187107</td>\n",
    "      <td>187361</td>\n",
    "      <td>187734</td>\n",
    "      <td>188105</td>\n",
    "      <td>156313</td>\n",
    "      <td>165729</td>\n",
    "      <td>169323</td>\n",
    "      <td>183393</td>\n",
    "      <td>180119</td>\n",
    "      <td>181177</td>\n",
    "      <td>186623</td>\n",
    "      <td>184110</td>\n",
    "      <td>172716</td>\n",
    "      <td>183991</td>\n",
    "      <td>187436</td>\n",
    "      <td>187617</td>\n",
    "      <td>188049</td>\n",
    "      <td>179127</td>\n",
    "      <td>137611</td>\n",
    "      <td>187071</td>\n",
    "      <td>179505</td>\n",
    "      <td>172949</td>\n",
    "      <td>183762</td>\n",
    "      <td>188173</td>\n",
    "      <td>188136</td>\n",
    "      <td>185296</td>\n",
    "      <td>188079</td>\n",
    "      <td>188018</td>\n",
    "      <td>187872</td>\n",
    "      <td>187596</td>\n",
    "      <td>188273</td>\n",
    "      <td>188239</td>\n",
    "      <td>188271</td>\n",
    "      <td>186056</td>\n",
    "      <td>179982</td>\n",
    "      <td>187626</td>\n",
    "      <td>188176</td>\n",
    "      <td>188011</td>\n",
    "      <td>188295</td>\n",
    "      <td>178646</td>\n",
    "      <td>118322</td>\n",
    "      <td>154275</td>\n",
    "      <td>184731</td>\n",
    "      <td>154307</td>\n",
    "      <td>181347</td>\n",
    "      <td>187503</td>\n",
    "      <td>186526</td>\n",
    "      <td>152929</td>\n",
    "      <td>137505</td>\n",
    "      <td>154385</td>\n",
    "      <td>147536</td>\n",
    "      <td>141534</td>\n",
    "      <td>154939</td>\n",
    "      <td>186005</td>\n",
    "      <td>103852</td>\n",
    "      <td>166992</td>\n",
    "      <td>168926</td>\n",
    "      <td>183744</td>\n",
    "      <td>177993</td>\n",
    "      <td>111028</td>\n",
    "      <td>124689</td>\n",
    "      <td>150237</td>\n",
    "      <td>121642</td>\n",
    "      <td>87531</td>\n",
    "      <td>174360</td>\n",
    "      <td>78127</td>\n",
    "      <td>105492</td>\n",
    "      <td>79455</td>\n",
    "      <td>42970</td>\n",
    "      <td>106721</td>\n",
    "      <td>177274</td>\n",
    "      <td>123737</td>\n",
    "      <td>42925</td>\n",
    "      <td>76493</td>\n",
    "      <td>47165</td>\n",
    "      <td>47310</td>\n",
    "      <td>65512</td>\n",
    "      <td>152918</td>\n",
    "      <td>25305</td>\n",
    "      <td>128395</td>\n",
    "      <td>25148</td>\n",
    "      <td>26191</td>\n",
    "      <td>131693</td>\n",
    "      <td>43866</td>\n",
    "      <td>21061</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'Loss':\n",
    "The 'Loss' attribute/feature or also the label which we'll try to predict, is considered its own separate data. The descriptive and skewness of the 'Loss' attribute/feature is shows below:\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg .tg-yw4l{vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\"><br></th>\n",
    "    <th class=\"tg-yw4l\">Loss<br></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">count</td>\n",
    "    <td class=\"tg-yw4l\">188318.000000</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">mean</td>\n",
    "    <td class=\"tg-yw4l\">3037.337686</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">std</td>\n",
    "    <td class=\"tg-yw4l\">2904.086186</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">min</td>\n",
    "    <td class=\"tg-yw4l\">0.670000</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">25%</td>\n",
    "    <td class=\"tg-yw4l\">1204.460000</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">50%</td>\n",
    "    <td class=\"tg-yw4l\">2115.570000</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">75%</td>\n",
    "    <td class=\"tg-yw4l\">3864.045000</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">max</td>\n",
    "    <td class=\"tg-yw4l\">121012.250000</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "![alt text](./Loss_skew.png \"Logo Title Text 1\")\n",
    "\n",
    "The values for the 'loss' feature are abnormally large and if we ever plan to feed em into a model, we must take into consideration that the data from 'loss' will have to be normalized. Now we'll go into further detail in future in regards to the type of normalizaiton we use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2 Exploratory Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continious Data \n",
    "\n",
    "One amazing way we can convey and understand the relationship among some continious varibale is using a heatmap/pairplot sort of plotting which can clearly explain the commonality among all features in a matrix.\n",
    "\n",
    "![alt text](./cont_corr.png \"Logo Title Text 1\")\n",
    "\n",
    "**Observations:**\n",
    "* There are quite a lot of features that strongly correlate with one another. For example: 'Cont11' & 'Cont12' are highly correlational. Same goes for 'Cont1' & 'Cont9'. and so on untill we have crossed the margin of 0.0. In which case those columns are not correlated to one another. \n",
    "* Dimension reduction would be an ideal strategy if the data one had was quite extensive/large, however since ours is not, we'll leave it as is for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data \n",
    "\n",
    "Categorical data by nature is data in terms of strings and alphabet. There are quite a lot of ways analysize categorical data. As is done by a fellow kaggler, [Allstate Feature Analysis by Achal](https://www.kaggle.com/achalshah/allstate-feature-analysis-python), you could also extract correlation data among categorical features. However, for my own analysis I only relied on a simple countplot which would display the unique occurence of each string and its count. \n",
    "\n",
    "**For visual images please refer to the main notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *'Loss'*\n",
    "\n",
    "The dependant varibale, also known as the label thats going to be predicted, can be visualzied in multiole ways. Other fellow kagglers have amazing descriptive visual analysis of this feature, however I will be explainiing my own approach.\n",
    "\n",
    "Since not a lot of visualization is necessary to get an idea of how 'loss' is represented, a simple visualization of its skew would be enough. \n",
    "\n",
    "The original Skew before transformation: \n",
    "   ![alt text](./Loss_Skew.png \"Logo Title Text 1\")\n",
    "    \n",
    "After log-transofrmation: \n",
    "   ![alt text](./after_transform.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.3 Algorithms and technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types: \n",
    " For this Challenge, as mentioned earlier the problem is a regression problem hence regression algorithms are my choice of algoithms/models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Description:\n",
    "\n",
    "* LinearRegression: This is a simple Linear regression. It takes in a train sample and attempts to minimize the error by trying to fit the model to the train set. \n",
    "\n",
    "* SGDRegressor: Unlike Linear Regression, SGD takes the gradient of it's error is computed, applied at each time step with a decreasing multiplicator/learing rate taken into consideration.\n",
    "\n",
    "* Decision Tree Regressor: A tree like graph in which a statistical probabiity is assigned to each node and thus each data that is fed in, outputs a prediction with each node/attribute in mind.\n",
    "\n",
    "* Random Forest Regressor: Can be considered a meta-DecisionTree which encompasses multiple classifying decison trees at each node, which inevitably improves the predictive accuracy of the model over a single Decision Tree.\n",
    "\n",
    "* GradientBoostingRegressor: An ensemble of decison trees that build on top of each other's previous error, summed and itertively minimzies the error untill a certain tolerance is reached. \n",
    "\n",
    "* MLPregressor = A mulitlayer perceptron which can be a mix of various inout_layers, hidden_layers, and output_layers. Input layers usually correlate with the amount of features an input training data set has, hidden_layers correlate with multple activation functions that compute a gradient and and output that is fed out to output_layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Parameters:\n",
    "\n",
    "###### Not a lot of parameter tunning was done to avoid overfitting since the data is not as noisy.\n",
    "\n",
    "* LinearRegression: Eevrything was kept as default, but n_jobs was changed to '-1' so that it can provide a speedup if the problem is large and you ask the algorithm to use more CPUs, but it will not change error measures.\n",
    "\n",
    "* SGDRegressor: Everything else was kept as default, except max_iter, tol, epsilon, and alpha just for to avoid the default settings and to introduce a bit randomnesss. The manual paramters did not change the error as much. \n",
    "\n",
    "* Decision Tree Regressor: Everything else was kept as default, however only the random_state parameter was set to 0, to improve consistency. \n",
    "\n",
    "* Random Forest Regressor: Everything was kept as default, however n_jobs was set to '-1' for reasons mentioned above, and most importantly the criterion had to manually be set to 'mae' to avoid a huge bug as mentioned in this link. https://mail.python.org/pipermail/scikit-learn/2016-October/000757.html which caused a huge time consumption for some reason. Hence why i chnaged to 'mse' despite the fact that the challenge did not permit it.\n",
    "\n",
    "* GradientBoostingRegressor: Everything was kept as default, except Random_state, n_estimators. n_estimators was manually set to 25 b/c the default value, 100, took an insame amount of time on a macbook and did not yield any significant error loss. \n",
    "\n",
    "* MLPregressor: Everything was kept as default, except Random_state to improve consistency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques\n",
    "\n",
    "1. **train_test_split**\n",
    "    * We will train our model on what we'll call the training set, a subset of the data that we have.\n",
    "    * To make sure our model generalises, we need to test it on some data it has not seen before and evaluate how well       it does predicting on that data.\n",
    "    * To do this, we need to set aside data for testing our model - the test set.\n",
    "    * The 'Loss' label column of our will be split into two sets of data, X_test, y_test.\n",
    "    * Every other feature will be split into two training sets, X_train, y_train.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Tranformation\n",
    "\n",
    " * Since our Loss feature value as mentioned above needs to be log-normalized, we should do that before we feed our data into the models.\n",
    " * This can prevent any abnormal behavior and ourright normalize the data that will be outputted. \n",
    " * The log transform formula is: ![alt text](./gif.png \"Log1p\")\n",
    " * Further detail on this transformation can be found here; https://scicomp.stackexchange.com/questions/20629/when-should-log1p-and-expm1-be-used\n",
    " * keep in mind, to return the data back to it's original form you have to multiply the test set, aka the split 'loss' labels, by expm1, aka its inverse function to get the original value back. ![alt text](./expm1.png \"Log1p\")\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding\n",
    "\n",
    " * In order for us to feed in the categorical data, we have to also transform the categorical data into a numerical data. \n",
    " * The most common method for such transformation is One-Hot Encoding. It is widely used and proven by many reaserchers to work very well and often improve machine learning as explained by another fellow data scientists at this link; https://stackoverflow.com/questions/17469835/why-does-one-hot-encoding-improve-machine-learning-performance\n",
    " * Anyways, I had to encode all the categorical data by first using the labelencoder, which in turn will turn our string data into numerical data. \n",
    " * Right after, the data must be reshaped from a column array into vector array so it can be fed into One_Hot encoder next.\n",
    " * next, we'll use the One-Hot encoding algorithm to output a sparse matrix in which where each column corresponds to one possible value of one feature. \n",
    " * This process will output a matrix that is suitable for most if not all scikit estimators, which is why we must always encode the charcther data ino numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2 Initial implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process\n",
    "\n",
    " 1. Imported algorithm\n",
    " 2. Initialized the algorithm\n",
    " 3. Fit and trained the model on the split training data; X_train, y_train.\n",
    " 4. I used the predict method available to the algorithm and used it on the X_test data.\n",
    " 5. I obtained the result of Mean Absolute Error algorithm by using the y_test as ground truth, and the prediction as my estimated lable. \n",
    " 6. I appended the score achived from this algorithm, along with the name of the alogrithm to a separate list for later visualizing and analyzing each algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This process was applied to each and every algorthm since they share a common library.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.4 Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remembering to transform back my previously log normalized 'loss' column\n",
    "   as mentioned before, I had to log normalize my labels and I initally forgot to transform them back upon feeding them into the Mean_absolute_error function hence Why abnormal results were produced.\n",
    "2. I decided to re-edit the linear regression model, in which it iteratively goes through each column successively, training, predicting, and summing the residual error. This prooved to be quite amazing in fixing the atrocious result I kept getting before in which the error was in 6-7 figures. The new error seems much more reasonable and less like an outlier. This is the only process by which the error remains reasonable. For the reviewer feel free to play around with the model and let me know if there are any better improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.1 Model Evaluation and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The final Model is:\n",
    "\n",
    "* Features: The test.csv was used in the final prediction process. With all features included, Categorical data encoded and continius data remained the same.\n",
    "\n",
    "* Classifier: The final choice of classifier was SGD. This was the shocking result.\n",
    "\n",
    "* Target: Predict Insurance claim severity, aka the value of the 'loss' column/feature.\n",
    "\n",
    "The model had the lowest mean absolute error across a 10 trails. Insight: Most of the improvements and complexer models I tried to make only made the model worse. This goes to show that simple soluions/models are just enough to solve a regression probelm and over-the-top complex models are unnecessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.2 Justification (Comparison with expectations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this model aligns with solution expectations and on average performed close to what other kaggle competitors have achieved. Shocking result was the fact that SGD came on top even if that margin was in the decimal places, however since there is no benchmark to compare our model to, the true ground truth labels, the results we have obtained are as is. That being said, the significant speed up of using SGD over other time extensive algo's such as XGBoost, random forest, and even MLP, make SGD a great contender. \n",
    "\n",
    "Despite the ambiguity in dataset and lack of benchmakr, I belive the model given the constraints performed very well, with  +/- 5% error margin on each trial proving to be quite consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.1 Free-Form Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting predictions compared with actual prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.2 Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
